{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JAOYEdsbJ2iL",
        "IQht2NTnJ3dc",
        "WcvKxsSdJ9wu",
        "DLhqy3sFS_Y-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-zaGn8ZJDc5"
      },
      "outputs": [],
      "source": [
        "# running the setup file containing neccesary libraries and functions\n",
        "%run 'notebooks/scripts/setup.ipynb'\n",
        "\n",
        "# import libraries to enable network operations\n",
        "import networkx as nx\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to construct Network 1. Narrative topic - Narrative topic Relation"
      ],
      "metadata": {
        "id": "JAOYEdsbJ2iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_network_1(df):\n",
        "\n",
        "  # create a dictionary to store words for each topic\n",
        "  topic_words_dict = {}\n",
        "  for index, row in df.iterrows():\n",
        "      topic_name = row['topic_theme']\n",
        "      words_in_topic = \" \".join([word_prob.split(\"'\")[1] for word_prob in row[:-1] if pd.notna(word_prob)])\n",
        "      topic_words_dict[topic_name] = words_in_topic\n",
        "\n",
        "  # create dictionary to store topic data\n",
        "  topics_data = {}\n",
        "  for index, row in df.iterrows():\n",
        "      topic_name = row['topic_theme']\n",
        "      words_probs = [ast.literal_eval(word_prob) for word_prob in row[:-1] if pd.notna(word_prob)]\n",
        "      words = [word_prob[0] for word_prob in words_probs]\n",
        "      probabilities = [float(word_prob[1]) for word_prob in words_probs]\n",
        "      topics_data[topic_name] = {'words': words, 'probabilities': probabilities}\n",
        "\n",
        "  # generate all possible pairs of topics from the words list\n",
        "  topic_pairs = list(combinations(list(topic_words_dict.keys()), 2))\n",
        "\n",
        "  # create the co-occurrence counter with the pairs and their summed probabilities\n",
        "  cooccurrence_counter = Counter()\n",
        "  for topic_pair in topic_pairs:\n",
        "    words_0 = topics_data[topic_pair[0]]['words']\n",
        "    words_1 = topics_data[topic_pair[1]]['words']\n",
        "\n",
        "    # find indexes of common elements between the two lists\n",
        "    common_indexes_0 = [index for index, value in enumerate(words_0) if value in words_1]\n",
        "    common_indexes_1 = [index for index, value in enumerate(words_1) if value in words_0]\n",
        "\n",
        "    common_probs_0 = [value for index, value in enumerate(topics_data[topic_pair[0]]['probabilities']) if index in common_indexes_0]\n",
        "    common_probs_1 = [value for index, value in enumerate(topics_data[topic_pair[1]]['probabilities']) if index in common_indexes_1]\n",
        "\n",
        "    cooccurrence_counter[topic_pair] = round(sum(common_probs_0) + sum(common_probs_1), 4)\n",
        "\n",
        "\n",
        "  # create graph\n",
        "  G = nx.Graph()\n",
        "  for topic in df['topic_theme'].unique():\n",
        "      G.add_node(topic)\n",
        "  for (topic1, topic2), weight in cooccurrence_counter.items():\n",
        "      G.add_edge(topic1, topic2, weight=weight)\n",
        "\n",
        "  return G"
      ],
      "metadata": {
        "id": "vbRdq-1FJOWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to construct Network 2. Class style - Class style Relation"
      ],
      "metadata": {
        "id": "IQht2NTnJ3dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_network_2(df):\n",
        "  df_topics_grouped = df[['class_style', 'dominant_topic_theme']].groupby(['class_style', 'dominant_topic_theme']).size().reset_index(name='count')\n",
        "  df_topics_grouped = df_topics_grouped[['class_style', 'dominant_topic_theme']]\n",
        "\n",
        "  style_by_topic_df = df_topics_grouped.groupby('class_style')['dominant_topic_theme'].apply(list).reset_index()\n",
        "\n",
        "  # create a dictionary to store the co-occurrence counts\n",
        "  cooccurrence_counts = {}\n",
        "  for style1, style2 in itertools.combinations(style_by_topic_df['class_style'], 2):\n",
        "      topics1 = set(style_by_topic_df[style_by_topic_df['class_style'] == style1]['dominant_topic_theme'].iloc[0])\n",
        "      topics2 = set(style_by_topic_df[style_by_topic_df['class_style'] == style2]['dominant_topic_theme'].iloc[0])\n",
        "      cooccurrence_count = len(topics1.intersection(topics2))\n",
        "      cooccurrence_counts[(style1, style2)] = cooccurrence_count\n",
        "\n",
        "  # create the graph\n",
        "  G = nx.Graph()\n",
        "  styles = style_by_topic_df['class_style'].unique()\n",
        "  style_pairs = list(itertools.combinations(styles, 2))\n",
        "  G.add_nodes_from(styles)\n",
        "\n",
        "  for pair in style_pairs:\n",
        "    if cooccurrence_counts[pair] > 0:\n",
        "      G.add_edge(pair[0], pair[1], weight= cooccurrence_counts[pair])\n",
        "\n",
        "  return G"
      ],
      "metadata": {
        "id": "X3Usdu1fJObu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to construct Network 3. Narrative topic - Class Style Relation"
      ],
      "metadata": {
        "id": "WcvKxsSdJ9wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_network_3(df, styles, topics):\n",
        "\n",
        "  df_style_topic_weight = df[['class_style', 'Balance and Alignment', 'Stress Relief/Breath Techniques', 'Deep Relaxation/Mind Practice', 'Stretch and Release', 'Core and Strength']]\n",
        "\n",
        "  by_style_mean = df[['class_style', 'Balance and Alignment', 'Stress Relief/Breath Techniques', 'Deep Relaxation/Mind Practice', 'Stretch and Release', 'Core and Strength']].groupby(['class_style'], as_index=False).mean()\n",
        "  np_by_style_mean = by_style_mean.to_numpy()\n",
        "\n",
        "  # create the graph\n",
        "  G = nx.Graph()\n",
        "  G.add_nodes_from(styles, bipartite='styles')\n",
        "  G.add_nodes_from(topics, bipartite='topics')\n",
        "\n",
        "  for i in np_by_style_mean:\n",
        "    for topicIndex, topicName in enumerate(topics):\n",
        "      G.add_edge(topicName, i[0], weight= i[1+topicIndex])\n",
        "\n",
        "  return G"
      ],
      "metadata": {
        "id": "S-suDfGsJ9-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_projection(graph, nodes):\n",
        "\n",
        "  def my_weight_sum(G, u, v, weight='weight'):\n",
        "      w = 0.0\n",
        "      for nbr in set(G[u]) & set(G[v]):\n",
        "        w += G[u][nbr].get(weight, 1) + G[v][nbr].get(weight, 1)\n",
        "      return w\n",
        "\n",
        "  if nodes == 'topics':\n",
        "    topic = [n for n, d in graph.nodes(data=True) if d['bipartite'] == 'topics']\n",
        "    G = nx.bipartite.generic_weighted_projected_graph(graph, topic, my_weight_sum)\n",
        "\n",
        "  if nodes == 'styles':\n",
        "    style = [n for n in graph.nodes() if graph.nodes[n]['bipartite'] == 'styles']\n",
        "    G = nx.bipartite.generic_weighted_projected_graph(graph, style, my_weight_sum)\n",
        "\n",
        "  return G\n",
        "\n",
        "def draw_projection(graph):\n",
        "  plt.figure(figsize=(15, 20))\n",
        "  nx.draw_networkx(graph, with_labels = True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qMIhxK1nJ-C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to calculate Network measures\n"
      ],
      "metadata": {
        "id": "Ziurmkd8KLA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For unipartite graphs\n"
      ],
      "metadata": {
        "id": "BGW5bvzkRVaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_centralities(graph):\n",
        "  centralities_dict = {}\n",
        "\n",
        "  degree_centralities = graph.degree(weight='weight')\n",
        "  betweenness_centralities = nx.betweenness_centrality(graph, weight='weight')\n",
        "  eigenvector_centralities = nx.eigenvector_centrality(graph, weight='weight')\n",
        "  closeness_centralities = dict(nx.closeness_centrality(graph, distance='weight')) # the more weight the closer the nodes\n",
        "\n",
        "  for node, centrality in dict(degree_centralities).items():\n",
        "    centralities_dict[node] = {'degree_centrality': round(centrality, 6)}\n",
        "  for node, centrality in betweenness_centralities.items():\n",
        "    centralities_dict[node]['betweenness_centrality'] = round(centrality, 6)\n",
        "  for node, centrality in eigenvector_centralities.items():\n",
        "    centralities_dict[node]['eigenvector_centrality'] = round(centrality, 6)\n",
        "  for node, centrality in closeness_centralities.items():\n",
        "    centralities_dict[node]['closeness_centrality'] = round(centrality, 6)\n",
        "\n",
        "  df = pd.DataFrame(centralities_dict).T\n",
        "  df.reset_index(inplace=True)\n",
        "  df.rename(columns={'index': 'node'}, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "def plot_centralities_distributions(df):\n",
        "  # Create histograms with rows for each column\n",
        "  fig, axes = plt.subplots(nrows=len(df.columns) - 1, ncols=1, figsize=(8, 12))\n",
        "  plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "  for i, column in enumerate(df.columns[1:], start=1):  # Exclude the 'Category' column\n",
        "      axes[i - 1].hist(df[column], bins=10, edgecolor='k')\n",
        "      axes[i - 1].set_title(f'Distribution of {column}')\n",
        "      axes[i - 1].set_xlabel(column)\n",
        "      axes[i - 1].set_ylabel('Frequency')\n",
        "      axes[i - 1].grid(True)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def calculate_clusters(graph):\n",
        "    df = pd.DataFrame(columns=['Cluster', 'Members'])\n",
        "    # use the Girvan-Newman algorithm to detect clusters\n",
        "    clusters = nx.algorithms.community.girvan_newman(graph)\n",
        "    cluster_number = []\n",
        "    members = []\n",
        "    for i, cluster in enumerate(clusters):\n",
        "      cluster_number.append(i+1)\n",
        "      members.append(cluster)\n",
        "    df['Cluster'] = cluster_number\n",
        "    df['Members'] = members\n",
        "\n",
        "    return df\n",
        "\n",
        "def detect_communities(graph):\n",
        "    df = pd.DataFrame(columns=['Community', 'Members'])\n",
        "    # apply the greedy modularity communities algorithm for community detection\n",
        "    communities = nx.algorithms.community.greedy_modularity_communities(graph)\n",
        "    community_number = []\n",
        "    members = []\n",
        "    for i, community in enumerate(communities):\n",
        "      community_number.append(i+1)\n",
        "      members.append(community)\n",
        "    df['Community'] = community_number\n",
        "    df['Members'] = members\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def calculate_clustering_coefficient(graph):\n",
        "    clustering_coefficient = nx.average_clustering(graph, weight='weight')\n",
        "    return round(clustering_coefficient, 6)\n",
        "\n",
        "def calculate_average_shortest_path(graph):\n",
        "    average_shortest_path = nx.average_shortest_path_length(graph, weight='weight')\n",
        "    return round(average_shortest_path, 6)\n",
        "\n",
        "def calculate_density(graph):\n",
        "    network_density = nx.density(graph)\n",
        "    return round(network_density, 6)\n",
        "\n",
        "def calculate_transitivity(graph):\n",
        "    network_transitivity = nx.transitivity(graph)\n",
        "    return round(network_transitivity, 6)\n",
        "\n",
        "def general_graph_statistics(graph):\n",
        "  df = pd.DataFrame(columns=['clustering_coefficient', 'average_shortest_path', 'network_density', 'network_transitivity'], index=['score'])\n",
        "  df['clustering_coefficient'] = calculate_clustering_coefficient(graph)\n",
        "  df['average_shortest_path'] = calculate_average_shortest_path(graph)\n",
        "  df['network_density'] = calculate_density(graph)\n",
        "  df['network_transitivity'] = calculate_transitivity(graph)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "EkniaDJTJ-GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For bipartite graphs"
      ],
      "metadata": {
        "id": "tQrnkDj6RY6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_centralities_bipartite(graph, nodes):\n",
        "  centralities_dict = {}\n",
        "\n",
        "  degS, degT = nx.algorithms.bipartite.degrees(graph, nodes, weight='weight')\n",
        "  betweenness_centralities = nx.algorithms.bipartite.betweenness_centrality(graph, nodes)\n",
        "  closeness_centralities = nx.algorithms.bipartite.closeness_centrality(graph, nodes)\n",
        "\n",
        "  for node, centrality in dict(degS).items():\n",
        "    centralities_dict[node] = {'degree_centrality': round(centrality, 6)}\n",
        "  for node, centrality in dict(degT).items():\n",
        "    centralities_dict[node] = {'degree_centrality': round(centrality, 6)}\n",
        "  for node, centrality in betweenness_centralities.items():\n",
        "    centralities_dict[node]['betweenness_centrality'] = round(centrality, 6)\n",
        "  for node, centrality in closeness_centralities.items():\n",
        "    centralities_dict[node]['closeness_centrality'] = round(centrality, 6)\n",
        "\n",
        "  df = pd.DataFrame(centralities_dict).T\n",
        "  df.reset_index(inplace=True)\n",
        "  df.rename(columns={'index': 'node'}, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "def calculate_clustering_coefficient_bipartite(graph):\n",
        "    clustering_coefficient = nx.algorithms.bipartite.average_clustering(graph)\n",
        "    return round(clustering_coefficient, 6)\n",
        "\n",
        "def calculate_density_bipartite(graph, nodes):\n",
        "    network_density = nx.algorithms.bipartite.density(graph, nodes)\n",
        "    return round(network_density, 6)\n",
        "\n",
        "\n",
        "# nx.algorithms.bipartite.minimum_weight_full_matching(graph_3, weight='weight')\n",
        "# or maximum matching\n",
        "# https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.bipartite.matching.minimum_weight_full_matching.html#networkx.algorithms.bipartite.matching.minimum_weight_full_matching"
      ],
      "metadata": {
        "id": "wjA-P-hFRT-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to save data"
      ],
      "metadata": {
        "id": "DLhqy3sFS_Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_to_csv(graph, filepath, filename):\n",
        "  df_graph = pd.DataFrame(columns=['source', 'target', 'weight'])\n",
        "  nodes1 = []\n",
        "  nodes2 = []\n",
        "  attributes = []\n",
        "  for n1, n2, attr in graph.edges(data=True):\n",
        "    nodes1.append(n1)\n",
        "    nodes2.append(n2)\n",
        "    attributes.append(attr['weight'])\n",
        "  df_graph['source'] = nodes1\n",
        "  df_graph['target'] = nodes2\n",
        "  df_graph['weight'] = attributes\n",
        "\n",
        "  df_graph.to_csv(f'{filepath}{filename}.csv')"
      ],
      "metadata": {
        "id": "cvVjFNdyS-r0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
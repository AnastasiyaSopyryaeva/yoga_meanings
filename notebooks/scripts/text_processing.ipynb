{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# access google drive folder\n",
        "%cd /content/drive/MyDrive/Yoga_Classes\n",
        "\n",
        "# running the setup file containing basic libraries and functions\n",
        "%run 'notebooks/scripts/setup.ipynb'\n",
        "\n",
        "# import natural language processing toolkit\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# import modules to enable text analysis\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# import modules to plot distributions\n",
        "from plotly.offline import plot\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "o3nsXTWuDTnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for text-*preprocessing*"
      ],
      "metadata": {
        "id": "FogGSdhmH74k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In order to prepare textual data for any further quantitative manipulation it has to be cleaned and transformed properly. The text preprocessing steps in this case are the following:\n",
        "* Cleaning: keep only alphanumerical letters (no punctuations, questions marks, tabs etc.)\n",
        "* Lowercase all words\n",
        "* Remove stop words from the texts. We used basic nltk english set of stopwords (very common english words) and defined domain-specific set of words (yoga/fitness class related words that do not bring any value for the analysis no matter how often they appear)\n",
        "* Lemmatize the words in the text. Lemmatization stands for bringing words to their basic form (so to take into account only semantic differences, not morphological)\n"
      ],
      "metadata": {
        "id": "lAOQiD4mIEWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus):\n",
        "    cleaned_corpus = pd.Series(dtype='object')\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "            p1 = p1.lower()\n",
        "            qs.append(p1)\n",
        "        cleaned_corpus = pd.concat([cleaned_corpus, pd.Series(' '.join(qs))])\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "Hrhr8nJtGCWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def stopwords_removal(corpus, stop_set=None):\n",
        "    corpus = [[x for x in x.split() if x not in stop_set] for x in corpus]\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "SwFpt7vSGFU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    corpus = [[lemmatizer.lemmatize(x, pos = 'v') for x in x.split()] for x in corpus]\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "ib6NzVynGHgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(corpus, cleaning = True, lemmatization = True, remove_stopwords = True, **kwargs):\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus)\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus, **kwargs)\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "EKZkwa3AGJnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for basic text-*exploration*"
      ],
      "metadata": {
        "id": "bEfEF4sQGZXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    df_unigrams = pd.DataFrame(words_freq[:n], columns = ['unigram' , 'count'])\n",
        "    return df_unigrams"
      ],
      "metadata": {
        "id": "jOUY1fyLGQcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_bigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    df_bigrams = pd.DataFrame(words_freq[:n], columns = ['bigram' , 'count'])\n",
        "    return df_bigrams"
      ],
      "metadata": {
        "id": "O3UnAmxkGRBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ngrams_distribution(df_ngram, ngram):\n",
        "\n",
        "  fig = go.Figure([go.Bar(x=df_ngram[ngram], y=df_ngram['count'])])\n",
        "  title = f\"Top 15 {ngram} in the corpus after pre-processing\"\n",
        "  fig.update_layout(title=go.layout.Title(text=title))\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "jLPwoAqGuGGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_cloud(text):\n",
        "  word_cloud = WordCloud(\n",
        "        width=3000,\n",
        "        height=2000,\n",
        "        random_state=1,\n",
        "        collocations=False,\n",
        "        stopwords=STOPWORDS,\n",
        "        ).generate(text)\n",
        "\n",
        "  plt.imshow(word_cloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "0KET1vZcGRD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_style_word_clouds(df, class_styles_list):\n",
        "\n",
        "  def get_style_descriptions_variable(df, class_style):\n",
        "    df = df[df['class_style'] == class_style]\n",
        "    texts = df['cleaned_text_with_styles']\n",
        "    text = \" \".join(var for var in texts)\n",
        "  return text\n",
        "\n",
        "  for i in class_styles_list:\n",
        "    class_text = get_style_descriptions_variable(df, i)\n",
        "    print(f'Word Cloud for {i} class_style')\n",
        "    create_word_cloud(class_text)\n",
        "    # save figure"
      ],
      "metadata": {
        "id": "PSLnY8kzvqNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(series, title, xlabel, ylabel, var_name, mean_line=True, median_line=True, bins=20):\n",
        "\n",
        "  series.hist(bins=bins)\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "\n",
        "  mean = round(series.mean())\n",
        "  median = round(series.median())\n",
        "  max = series.max()\n",
        "  min = series.min()\n",
        "\n",
        "  print(f'The shortest {var_name} has {min} words in total')\n",
        "  print(f'The longest {var_name} has {max} words in total')\n",
        "  print(f'The mean {var_name} length is {mean} words')\n",
        "  print(f'The median {var_name} length is {median} words')\n",
        "\n",
        "  if mean_line: #show mean line\n",
        "    plt.axvline(mean, color='k', linestyle='dashed', linewidth=1, label = f'mean: {mean}')\n",
        "\n",
        "  if median_line: #show median line\n",
        "    plt.axvline(median, color='r', linestyle='-', linewidth=1, label = f'median: {median}')\n",
        "\n",
        "  plt.legend(loc=\"upper right\")"
      ],
      "metadata": {
        "id": "T2VGAnd7YUT6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}